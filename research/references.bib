@article{LARADJI2015388,
  title    = {Software defect prediction using ensemble learning on selected features},
  journal  = {Information and Software Technology},
  volume   = {58},
  pages    = {388-402},
  year     = {2015},
  issn     = {0950-5849},
  doi      = {https://doi.org/10.1016/j.infsof.2014.07.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950584914001591},
  author   = {Issam H. Laradji and Mohammad Alshayeb and Lahouari Ghouti},
  keywords = {Defect prediction, Ensemble learning, Software quality, Feature selection, Data imbalance, Feature redundancy/correlation},
  abstract = {Context
              Several issues hinder software defect data including redundancy, correlation, feature irrelevance and missing samples. It is also hard to ensure balanced distribution between data pertaining to defective and non-defective software. In most experimental cases, data related to the latter software class is dominantly present in the dataset.
              Objective
              The objectives of this paper are to demonstrate the positive effects of combining feature selection and ensemble learning on the performance of defect classification. Along with efficient feature selection, a new two-variant (with and without feature selection) ensemble learning algorithm is proposed to provide robustness to both data imbalance and feature redundancy.
              Method
              We carefully combine selected ensemble learning models with efficient feature selection to address these issues and mitigate their effects on the defect classification performance.
              Results
              Forward selection showed that only few features contribute to high area under the receiver-operating curve (AUC). On the tested datasets, greedy forward selection (GFS) method outperformed other feature selection techniques such as Pearson’s correlation. This suggests that features are highly unstable. However, ensemble learners like random forests and the proposed algorithm, average probability ensemble (APE), are not as affected by poor features as in the case of weighted support vector machines (W-SVMs). Moreover, the APE model combined with greedy forward selection (enhanced APE) achieved AUC values of approximately 1.0 for the NASA datasets: PC2, PC4, and MC1.
              Conclusion
              This paper shows that features of a software dataset must be carefully selected for accurate classification of defective components. Furthermore, tackling the software data issues, mentioned above, with the proposed combined learning model resulted in remarkable classification performance paving the way for successful quality control.}
}

@article{dada2021ensemble,
  author  = {Lear, Adv and Dada, Emmanuel and Oyewola, David and Joseph, Stephen and Dauda, Ali and Bassi, Stephen and Baba, Ali},
  year    = {2021},
  month   = {07},
  pages   = {11-21},
  title   = {Ensemble Machine Learning Model for Software Defect Prediction},
  journal = {Advances in Machine Learning and Artificial Intelligence},
  volume  = {2}
}

@inproceedings{Akosa2017PredictiveA,
  title  = {Predictive Accuracy : A Misleading Performance Measure for Highly Imbalanced Data},
  author = {Josephine Sarpong Akosa},
  year   = {2017},
  url    = {https://api.semanticscholar.org/CorpusID:43504747}
}


@article{708428,
  author  = {Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal = {IEEE Intelligent Systems and their Applications},
  title   = {Support vector machines},
  year    = {1998},
  volume  = {13},
  number  = {4},
  pages   = {18-28},
  doi     = {10.1109/5254.708428}
}

@inproceedings{10.1007/BFb0020217,
  author    = {Sch{\"o}lkopf, Bernhard
               and Smola, Alexander
               and M{\"u}ller, Klaus-Robert},
  editor    = {Gerstner, Wulfram
               and Germond, Alain
               and Hasler, Martin
               and Nicoud, Jean-Daniel},
  title     = {Kernel principal component analysis},
  booktitle = {Artificial Neural Networks --- ICANN'97},
  year      = {1997},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {583--588},
  abstract  = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
  isbn      = {978-3-540-69620-9}
}

@article{Cortes1995,
  author   = {Cortes, Corinna and Vapnik, Vladimir},
  title    = {Support-vector networks},
  journal  = {Machine Learning},
  year     = {1995},
  volume   = {20},
  number   = {3},
  pages    = {273--297},
  month    = sep,
  doi      = {10.1007/BF00994018},
  url      = {https://doi.org/10.1007/BF00994018},
  issn     = {1573-0565},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimensional feature space. In this feature space, a linear decision surface is constructed. Special properties of the decision surface ensure the high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.}
}


@article{Breiman2001,
  author  = {Breiman, Leo},
  title   = {Random Forests},
  journal = {Machine Learning},
  year    = {2001},
  volume  = {45},
  number  = {1},
  pages   = {5--32}
}

@inproceedings{Chen2004,
  author    = {Chen, Chih-Chung and Liaw, Andy and Breiman, Leo},
  title     = {Using Random Forest to Learn Imbalanced Data},
  booktitle = {University of California, Berkeley},
  year      = {2004},
  pages     = {1--12}
}

@inproceedings{Dietterich2000,
  author    = {Dietterich, Thomas G.},
  title     = {Ensemble Methods in Machine Learning},
  booktitle = {International Workshop on Multiple Classifier Systems},
  year      = {2000},
  pages     = {1--15}
}

@article{DiazUriarte2006,
  author  = {Díaz-Uriarte, Ramón and De Andres, S. Amelia},
  title   = {Gene Selection and Classification of Microarray Data using Random Forest},
  journal = {BMC Bioinformatics},
  year    = {2006},
  volume  = {7},
  number  = {1},
  pages   = {1--13}
}

@article{Sagi2018,
  author  = {Sagi, Omer and Rokach, Lior},
  title   = {Ensemble Learning: A Survey},
  journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year    = {2018},
  volume  = {8},
  number  = {4},
  pages   = {e1249}
}
@book{hosmer2013applied,
  title     = {Applied logistic regression},
  author    = {Hosmer Jr, David W and Lemeshow, Stanley and Sturdivant, Rodney X},
  year      = {2013},
  publisher = {John Wiley \& Sons}
}
@article{menard2002applied,
  title     = {Applied logistic regression analysis},
  author    = {Menard, Scott},
  journal   = {Sage},
  year      = {2002},
  publisher = {Thousand Oaks, CA}
}
@article{peng2002introduction,
  title     = {An introduction to logistic regression analysis and reporting},
  author    = {Peng, Chao-Ying Joanne and Lee, Kristin L and Ingersoll, Gary M},
  journal   = {The journal of educational research},
  volume    = {96},
  number    = {1},
  pages     = {3--14},
  year      = {2002},
  publisher = {Taylor \& Francis}
}

@article{powers2011evaluation,
  title   = {Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation},
  author  = {Powers, David Martin},
  journal = {arXiv preprint arXiv:2010.16061},
  year    = {2011}
}
@article{sokolova2009systematic,
  title     = {A systematic analysis of performance measures for classification tasks},
  author    = {Sokolova, Marina and Lapalme, Guy},
  journal   = {Information Processing \& Management},
  volume    = {45},
  number    = {4},
  pages     = {427--437},
  year      = {2009},
  publisher = {Elsevier}
}
@book{van1979information,
  title     = {Information retrieval},
  author    = {Van Rijsbergen, Cornelis Joost},
  year      = {1979},
  publisher = {Butterworth-Heinemann}
}
@article{fawcett2006introduction,
  title     = {An introduction to ROC analysis},
  author    = {Fawcett, Tom},
  journal   = {Pattern recognition letters},
  volume    = {27},
  number    = {8},
  pages     = {861--874},
  year      = {2006},
  publisher = {Elsevier}
}

@article{dietterich2000ensemble,
  title     = {Ensemble Methods in Machine Learning},
  author    = {Dietterich, Thomas G},
  journal   = {Multiple classifier systems},
  volume    = {1857},
  pages     = {1--15},
  year      = {2000},
  publisher = {Springer}
}
@article{wolpert1992stacked,
  title     = {Stacked generalization},
  author    = {Wolpert, David H},
  journal   = {Neural networks},
  volume    = {5},
  number    = {2},
  pages     = {241--259},
  year      = {1992},
  publisher = {Elsevier}
}
@article{opitz1999popular,
  title     = {Popular ensemble methods: An empirical study},
  author    = {Opitz, David and Maclin, Richard},
  journal   = {Journal of artificial intelligence research},
  volume    = {11},
  pages     = {169--198},
  year      = {1999},
  publisher = {AI Access Foundation}
}
@book{zhou2012ensemble,
  title     = {Ensemble methods: foundations and algorithms},
  author    = {Zhou, Zhi-Hua},
  year      = {2012},
  publisher = {CRC press}
}
@article{rokach2010ensemble,
  title     = {Ensemble-based classifiers},
  author    = {Rokach, Lior},
  journal   = {Artificial Intelligence Review},
  volume    = {33},
  number    = {1-2},
  pages     = {1--39},
  year      = {2010},
  publisher = {Springer}
}

@online{sharma2022logistic,
  author  = {Sharma, H.},
  title   = {Logistic Regression - Python Implementation from Scratch without using sklearn},
  year    = {2022},
  month   = jul,
  day     = {6},
  url     = {https://heena-sharma.medium.com/logistic-regression-python-implementation-from-scratch-without-using-sklearn-d3fca7d3dae7},
  urldate = {2023-07-17}
}

@article{guyon2003introduction,
  title   = {An introduction to variable and feature selection},
  author  = {Guyon, Isabelle and Elisseeff, Andr{\'e}},
  journal = {Journal of machine learning research},
  volume  = {3},
  number  = {Mar},
  pages   = {1157--1182},
  year    = {2003}
}

@inproceedings{kira1992feature,
  title     = {The feature selection problem: Traditional methods and a new algorithm},
  author    = {Kira, Kenji and Rendell, Larry A},
  booktitle = {AAAI},
  volume    = {2},
  pages     = {129--134},
  year      = {1992}
}

@book{liu2007computational,
  title     = {Computational methods of feature selection},
  author    = {Liu, Huan and Motoda, Hiroshi},
  year      = {2007},
  publisher = {CRC Press}
}


@article{chawla2002smote,
  title     = {SMOTE: synthetic minority over-sampling technique},
  author    = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal   = {Journal of artificial intelligence research},
  volume    = {16},
  pages     = {321--357},
  year      = {2002},
  publisher = {Jair}
}

@article{fernandez2018smote,
  title     = {SMOTE for learning from imbalanced data: progress and challenges, marking the 15-year anniversary},
  author    = {Fern{\'a}ndez, Alberto and Garc{\'i}a, Salvador and Herrera, Francisco and Chawla, Nitesh V},
  journal   = {Journal of artificial intelligence research},
  volume    = {61},
  pages     = {863--905},
  year      = {2018},
  publisher = {Jair}
}
@article{bergstra2012random,
  title   = {Random search for hyper-parameter optimization},
  author  = {Bergstra, James and Bengio, Yoshua},
  journal = {Journal of Machine Learning Research},
  volume  = {13},
  number  = {Feb},
  pages   = {281--305},
  year    = {2012}
}

@inproceedings{kohavi1995,
  title     = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
  author    = {Kohavi, R.},
  booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2},
  year      = {1995}
}

@article{guyon2002,
  title   = {Gene Selection for Cancer Classification using Support Vector Machines},
  author  = {Guyon, I. and Weston, J. and Barnhill, S. and Vapnik, V.},
  journal = {Machine Learning},
  volume  = {46},
  number  = {1-3},
  pages   = {389--422},
  year    = {2002}
}
@article{10.3389/fbioe.2020.00496,
  author   = {Chen, Qi and Meng, Zhaopeng and Su, Ran},
  title    = {WERFE: A Gene Selection Algorithm Based on Recursive Feature Elimination and Ensemble Strategy},
  journal  = {Frontiers in Bioengineering and Biotechnology},
  volume   = {8},
  year     = {2020},
  url      = {https://www.frontiersin.org/articles/10.3389/fbioe.2020.00496},
  doi      = {10.3389/fbioe.2020.00496},
  issn     = {2296-4185},
  abstract = {Gene selection algorithm in micro-array data classification problem finds a small set of genes which are most informative and distinctive. A well-performed gene selection algorithm should pick a set of genes that achieve high performance and the size of this gene set should be as small as possible. Many of the existing gene selection algorithms suffer from either low performance or large size. In this study, we propose a wrapper gene selection approach, named WERFE, within a recursive feature elimination (RFE) framework to make the classification more efficient. This WERFE employs an ensemble strategy, takes advantages of a variety of gene selection methods and assembles the top selected genes in each approach as the final gene subset. By integrating multiple gene selection algorithms, the optimal gene subset is determined through prioritizing the more important genes selected by each gene selection method and a more discriminative and compact gene subset can be selected. Experimental results show that the proposed method can achieve state-of-the-art performance.}
}

@article{kohavi1997,
  title   = {Wrappers for feature subset selection},
  author  = {Kohavi, R. and John, G. H.},
  journal = {Artificial Intelligence},
  volume  = {97},
  number  = {1-2},
  pages   = {273--324},
  year    = {1997}
}

@article{10.1016/j.eswa.2012.02.053,
  author  = {Delany, S. and Buckley, M. and Greene, D.},
  title   = {Sms spam filtering: methods and data},
  journal = {Expert Systems With Applications},
  year    = {2012},
  volume  = {39},
  issue   = {10},
  pages   = {9899-9908},
  doi     = {10.1016/j.eswa.2012.02.053}
}

@book{mackay2003information,
  title     = {Information theory, inference and learning algorithms},
  author    = {MacKay, David JC},
  year      = {2003},
  publisher = {Cambridge university press}
}